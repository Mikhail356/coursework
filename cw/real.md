# Реализация макета децентрализованной системы
___
это раздел с детальным описание вашего проекта, включая содержимое файла README.

в реализацию макета, в принципе, можно добавить листинги, но сильно их урезать, без import, разбора sys.argv и т.п., может быть даже без обработки ошибок — только суть
___


## Основные рабочие объекты 
1. Страница ученого - html страница содержащая информацию о ученом, его соавторах и публикациях.
2. Страница публикации - html страница содержащая информацию о публикации и ее авторах.
3. Сервер - сервер, которым владеет определенное научное учреждение. На нем содержатся страницы ученых и их публикаций.
4. Источник (source) - страница ученого/публикации с которой отправляют web интернет запросы.
5. Цель (target) - страница ученого/публикации на которую поступают web интернет запросы.
4. WebMention сервер - специальный сервер, который получает и записывает в базу данных запросы от серверов_ в определенном формате: URL адрес source источника, URL адрес target цели, URL адрес WebMention сервера. Отличается своей структурой от сервера, но также является собственностью соответствующего серверу научного учреждения.
6. База Данных (БД) - хранит записи в формате: URL адрес source источника, URL адрес target цели, время поступления запроса на WebMention сервер, URL адрес WebMention сервера. Является собственностью соответствующего серверу и WebMention серверу научного учреждения.

## Команды работы с проектом
Команды для работы с уже развернутой системой.
### Команда обновления (update)
#### Кратко 
На вход подается URL адрес страницы. Команда отпраляет все ссылки в ней на WebMention сервер, который записывает их в соответствующем формате в БД. Используется перед удалением страницы или после изменения содержимого страницы, не затрагивающего ссылки.
#### Алгоритм работы 
1. Отправить GET запрос на указанный URL адрес
2. Если ответ пришел без ошибок (с кодом 200), то перейти к следующему шагу. Иначе прекратить работу и вывести сообщение об ошибке.
3. Разобрать html код страницы, полученный на прошлом шаге.
4. Найти все ссылки на другие страницы и URL адрес WebMention сервера.
5. Отправить на URL адрес WebMention сервера POST запросы, содержащие:
    1. URL Адрес личного  WebMention сервера для сервера, содержащего текущую страницу.
    1. URL Адрес текущей страницы.
    1. URL Адрес целевой страницы.
6. Вывод URL адресов обновленных страниц на экран.

#### Код на языке программирования Python (упрощенный)
```python
try:
  requests.get(sys.argv[1])
except OSError:
  print('Not correct url')
  sys.exit()
obj = mf2py.Parser(url=sys.argv[1]).to_dict()
b = obj['items'][1]['children']
l = len(b)
for i in range(l):
  t_url = b[i]['properties']['url'][0]
  wm_serv = mf2py.Parser(url=t_url).to_dict()
  wm_serv = wm_serv['rels']['webmention'][0]
  requests.post(wm_serv, 
                data = {'source' : sys.argv[1], 
                        'target' : t_url})
```

### Команда проверки (check)
#### Кратко 
На вход подается физический адрес .html файла страницы и ее URL адрес. Команда скачивает из БД свои запросы, появившиеся после последней проверки, и редактирует существующую страницу в соответствии с ними. Редактирование происходит путем добавления и удаления новых ученых, публикаций, ссылок. Используется после изменения ссылок внутри страницы.
#### Алгоритм работы 
1. Считать из отдельного файла (с именем time) время последнего обновления страницы.
2. Разобрать .html страницу на теги. 
3. Найти в разборе URL адрес WebMention сервера.
4. Считать из БД все записи, посланные на данный WebMention сервер, имеющие целью текущую страницу, и появившиеся после даты последней проверки.
5. Отправить GET запросы на все WebMention сервера с целью на страницы, запросы которых содержали целью текущую страницу.
6. Если результат запроса оказался удовлетворительным (вернулся с кодом 200) перейти к следующему пункту, иначе поместить обрабатываемую страницу в список адресов страниц на удаление (с именем remove).
7. Провести проверку страницы на подлинность, в случае успеха перейти к следующему шагу, иначе поместить данную страницу в список адресов страниц на удаление (с именем remove).
8. URL адрес страницы поместить в массив адресов страниц для добавления (с именем add).
9. Найти в разборе текущей страницы все ссылки на другие страницы и поместить в массив current.
10. Если среди последних есть адреса страниц подлежащих удалению (находятся в remove), то сделать соответствующие виртуальные изменения в коде страницы.
11. Если в current нет каких-то URL адресов из массива add, то сделать соответствующие виртуальные изменения в коде страницы.
12. Вывести на экран список изменений страницы и запросить пользователя разрешение на их внесение в физический код страницы.
13. В случае отказа, завершить работу команды. Иначе провести изменения кода страницы.
14. Записать в файл time время последнего обновления данной страницы.
15. Вызвать команду update, описанную выше.

#### Код на языке программирования Python (упрощенный)
```python
#----Проверка базы данных и составление списка кортежей из нее----
db = sqlite3.connect('webmention-logger.sqlite')
cursor = db.cursor()
cursor.execute("SELECT * FROM webmentions")
record = cursor.fetchall()
cursor.close()

# чтение файла с датой последней проверки
s = sys.argv[1]
s = s.split('/')
way, i ='', 0
while i != len(s)-1:
  way += s[i]
  way += '/'
  i+=1
way += 'd_check'

with open(way, 'r+') as f:
  datetime = f.readline()
datetime = parse(datetime)

#  подготовка таблицы
for i in record:
  if parse(i[3]) < datetime:
    record.remove(i)


with open(sys.argv[1]) as fp:
    soup = bs4.BeautifulSoup(fp, features="html5lib")

# Проверка всех новых обращений к текущей странице
# и добавление в список всех откликнувшихся страниц

# Поиск URL адреса WebMention сервера
wm = soup.find_all('link')
for i in wm:
  if i['rel'][0] == 'webmention':
    wm = i['href']
    break
else:
  print('Who is webmention server?')
  sys.exit()

#"Разделение" обращений удаления и добавления
cur_link = sys.argv[2]
to_add, to_remove = [], []
for i in record:
  if i[2] == cur_link and 'http://'+i[-1] == wm:
    try:
      fil = requests.get(i[1])
      fil.encoding = "html5lib"
      tmp = bs4.BeautifulSoup(fil.text, features="html5lib")
      tmp = tmp.title.text.split()
      name = tmp[0]+' '+tmp[1][0]+'. '+tmp[2][0]+'.'
      to_add.append((i[1], name))
    except OSError:
      to_remove.append(i[1])

# итоговые списки для изменения данных страницы это to_add to_remove
print('I want to change this part:','\t to_add:', to_add, 
                                    '\t to_remove:', to_remove, 
                                    sep = '\n')
print("(Y/N):", end = ' ')
ans = str(input()).lower()
# Обработака ответа пользователя в соответствии с алгоритмом
if(ans == 'y'):
  step = soup.find(class_ = "h-istina-coauthors")
  find = step.find_all(class_ = 'p-name u-url')

  if len(to_add) > 0:
    for i in to_add:
      for j in find:
        check = j.contents[0][0:-1]
        if i[1] == check:
          j['href'] = i[0]
          break
      else:
        upd = find[-1].contents[0][0:-1]+','
        find[-1].string = upd
        add = bs4.BeautifulSoup('<span class=\"h-card\"></span>', 
                                features = 'html5lib')
        an = add.span
        new_tag = add.new_tag("a", class_="p-name u-url", href = i[0])
        an.append(new_tag)
        add.span.a.string = i[1]+'.'
        add.span.a.attrs = {'class':'p-name u-url', 
                            'href': add.span.a.attrs['href']}
        step.append(add.span)

  if len(to_remove) > 0:
    find = step.find_all(class_ = 'p-name u-url')
    for i in to_remove:
      for j in find:
        check = j.contents[0][0:-1]
        if i[1] == check:
          j['href'] = ' '
          print(check, j.contents[0][0:-1], find)
  print(soup.prettify(formatter='html5'))

  with open(sys.argv[1], 'w') as fp:
    fp.writelines(str(soup.prettify(formatter='html5')))
  with open(way, 'w') as f:
    f.write(record[-1][3])
```